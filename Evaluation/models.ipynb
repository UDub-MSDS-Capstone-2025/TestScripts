{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Relevance** - Does the chatbot’s response align with the conversation context?\n",
    "    - Metric: Semantic similarity between user prompt and chatbot response. Compute cosine similarity of embeddings. Higher similarity = Higher relevance score.\n",
    "    - Implementation: Utilize sentence embeddings from models like sentence-transformers/all-mpnet-base-v2  (backup: sentence-transformers/all-MiniLM-L6-v2) to compute cosine similarity between the query and response embeddings.\n",
    "\n",
    "2. **Coherence** - Is the conversation logically structured?\n",
    "    - Metric: Sentence Embedding Similarity (Transformer-Based) measuring semantic coherence between consecutive bot-human conversations. ￼\n",
    "    - Implementation: Use sentence-transformers/all-mpnet-base-v2.\n",
    "\n",
    "3. **Factual Accuracy** - Are the chatbot’s statements correct and verifiable?\n",
    "    - Metric: Question Answering (QA) models: fact-checking against a knowledge base. Or Natural Language Inference (NLI) models: Determines if response contradicts factual statements. Higher entailment probability = Higher factual accuracy score.\n",
    "    - Implementation:  Employ models like facebook/bart-large-mnli (Strong NLI model) to verify facts.\n",
    "\n",
    "4. **Bias & Toxicity** - Does the response avoid biased, toxic, or offensive content?\n",
    "    - Metric: Toxicity classification: Score toxic and biased phrases in chatbot output. Lower toxicity = Higher score.\n",
    "\t- Implementation: Use models like unitary/toxic-bert (best for bias & toxicity detection) to detect toxic language.\n",
    "\n",
    "5. **Fluency** - Are responses grammatically correct and readable?\n",
    "    - Metric: Grammaticality score using a fluency-checking model. \n",
    "\t- Implementation: Utilize models like textattack/roberta-base-CoLA to check grammatical correctness directly.\n",
    "\n",
    "6. **Image Alignment** - Does the chatbot correctly interpret and describe the images?\n",
    "    - Metric: Vision-language similarity: Measures how well the response text aligns with the given image. Higher alignment score = Better rating. ￼\n",
    "    - Implementation: Use models like openai/clip-vit-base-patch32 (Multimodal model for vision-text matching).\n",
    "\n",
    "7. **Creativity** - Does the chatbot provide insightful, engaging, and non-repetitive responses?\n",
    "    - Metric: Lexical diversity measures how many unique words appear relative to total words. Higher diversity = Higher creativity score.\n",
    "    - Implementation: sentence-transformers/all-mpnet-base-v2 (Can generate diverse embeddings for novelty scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers sentence-transformers detoxify torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinyilyu/Documents/UW/Capstone Project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline, CLIPProcessor, CLIPModel\n",
    "from detoxify import Detoxify\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Load Transformer Models\n",
    "similarity_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")  # Relevance, Coherence\n",
    "fact_checker = pipeline(\"text-classification\", model=\"facebook/bart-large-mnli\")  # Factual Accuracy\n",
    "toxicity_model = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")  # Bias & Toxicity\n",
    "fluency_model = pipeline(\"text-classification\", model=\"textattack/roberta-base-CoLA\")  # Fluency\n",
    "vision_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")  # Image Alignment\n",
    "vision_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")  # CLIP Processor        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(user_prompts, bot_responses):\n",
    "    \"\"\"\n",
    "    Measures how well the chatbot's responses align with the overall conversation context.\n",
    "    \"\"\"\n",
    "    if not bot_responses or not user_prompts:\n",
    "        return 5, \"No responses provided.\"\n",
    "\n",
    "    # Compute embeddings for all user prompts and bot responses\n",
    "    embeddings_prompt = similarity_model.encode(user_prompts, convert_to_tensor=True)\n",
    "    embeddings_response = similarity_model.encode(bot_responses, convert_to_tensor=True)\n",
    "    similarity_score = util.pytorch_cos_sim(embeddings_prompt.mean(dim=0), embeddings_response.mean(dim=0)).item() * 10\n",
    "\n",
    "    return round(similarity_score), f\"Semantic relevance score: {similarity_score:.2f}\"\n",
    "\n",
    "def evaluate_coherence(bot_responses):\n",
    "    \"\"\"\n",
    "    Measures coherence by computing **average sentence similarity** across chatbot responses.\n",
    "    \"\"\"\n",
    "    if len(bot_responses) < 2:\n",
    "        return 5, \"Not enough chatbot responses to measure coherence.\"\n",
    "\n",
    "    # Compute coherence based on similarity between consecutive bot responses\n",
    "    similarities = []\n",
    "    for i in range(len(bot_responses) - 1):\n",
    "        emb1 = similarity_model.encode(bot_responses[i], convert_to_tensor=True)\n",
    "        emb2 = similarity_model.encode(bot_responses[i + 1], convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(emb1, emb2).item()\n",
    "        similarities.append(similarity)\n",
    "    avg_similarity = sum(similarities) / len(similarities) if similarities else 0.5\n",
    "    scaled_score = int(1 + (avg_similarity * 9))  # Min-Max scaling\n",
    "\n",
    "    return scaled_score, f\"Average bot response coherence: {avg_similarity:.2f}\"\n",
    "\n",
    "def evaluate_factual_accuracy(bot_responses):\n",
    "    \"\"\"\n",
    "    Uses NLI to check factual consistency across the entire conversation.\n",
    "    \"\"\"\n",
    "    if not bot_responses:\n",
    "        return 5, \"No responses provided.\"\n",
    "    \n",
    "    result = fact_checker(\" \".join(bot_responses))\n",
    "    entailment_score = max(result, key=lambda x: x['score'])['score'] * 10\n",
    "\n",
    "    return round(entailment_score), f\"Factual consistency score: {entailment_score:.2f}\"\n",
    "\n",
    "def evaluate_bias_toxicity(bot_responses):\n",
    "    \"\"\"\n",
    "    Detects bias or toxic language in chatbot responses.\n",
    "    \"\"\"\n",
    "    if not bot_responses:\n",
    "        return 10, \"No responses provided.\"\n",
    "    \n",
    "    combined_text = \" \".join(bot_responses)\n",
    "    toxicity_score = Detoxify('original').predict(combined_text)['toxicity'] * 10\n",
    "    final_score = max(1, 10 - int(toxicity_score))  # Lower toxicity = higher score\n",
    "\n",
    "    return final_score, f\"Toxicity probability: {toxicity_score:.2f}\"\n",
    "\n",
    "def evaluate_fluency(bot_responses):\n",
    "    \"\"\"\n",
    "    Uses CoLA model to check grammatical fluency across chatbot responses.\n",
    "    \"\"\"\n",
    "    if not bot_responses:\n",
    "        return 5, \"No responses provided.\"\n",
    "\n",
    "    fluency_scores = [fluency_model(response)[0]['score'] * 10 for response in bot_responses]\n",
    "    avg_fluency = sum(fluency_scores) / len(fluency_scores)\n",
    "\n",
    "    return round(avg_fluency), f\"Average fluency score: {avg_fluency:.2f}\"\n",
    "\n",
    "def evaluate_image_alignment(bot_responses, image_data, image_tag_mapping):\n",
    "    \"\"\"\n",
    "    Uses CLIP to check if chatbot responses align with encoded images.\n",
    "    \"\"\"\n",
    "    if not bot_responses or not image_data:\n",
    "        return 5, \"No images or responses provided.\"\n",
    "\n",
    "    scores = []\n",
    "    processed_texts = []\n",
    "    processed_images = []\n",
    "\n",
    "    # Process each image referenced in the conversation\n",
    "    for tag, img_name in image_tag_mapping.items():\n",
    "        for img in image_data:\n",
    "            if img[\"name\"] == img_name:\n",
    "                # Decode base64 image\n",
    "                image = Image.open(io.BytesIO(base64.b64decode(img[\"base64\"])))\n",
    "                \n",
    "                processed_images.append(image)\n",
    "                processed_texts.append(\" \".join(bot_responses))  # Combine bot responses\n",
    "\n",
    "    if not processed_images:\n",
    "        return 5, \"No valid images found for evaluation.\"\n",
    "\n",
    "    # Process text and images using CLIP with truncation/padding\n",
    "    inputs = vision_processor(\n",
    "        text=processed_texts, \n",
    "        images=processed_images, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the correct device\n",
    "    inputs = {key: val.to(vision_model.device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = vision_model(**inputs)\n",
    "        similarity_scores = outputs.logits_per_image.cpu().numpy()  # Convert to NumPy\n",
    "\n",
    "    # Compute Min-Max normalization for similarity scores\n",
    "    min_clip_score, max_clip_score = 0.1, 0.9  # Adjust based on dataset range\n",
    "    avg_score = similarity_scores.mean() if similarity_scores.size > 0 else 0.5\n",
    "    normalized_score = int(1 + ((avg_score - min_clip_score) / (max_clip_score - min_clip_score)) * 9)\n",
    "\n",
    "    return normalized_score, f\"CLIP text-image similarity score: {avg_score:.2f}\"\n",
    "\n",
    "def evaluate_creativity(bot_responses, past_responses):\n",
    "    \"\"\"\n",
    "    Measures creativity by comparing bot responses against past responses.\n",
    "    \"\"\"\n",
    "    if not bot_responses:\n",
    "        return 5, \"No responses provided.\"\n",
    "    \n",
    "    embedding_current = similarity_model.encode(bot_responses, convert_to_tensor=True)\n",
    "    embedding_past = similarity_model.encode(past_responses, convert_to_tensor=True) if past_responses else None\n",
    "\n",
    "    max_similarity = 0 if embedding_past is None else util.pytorch_cos_sim(embedding_current.mean(dim=0), embedding_past.mean(dim=0)).item()\n",
    "    creativity_score = max(1, min(10, int((1 - max_similarity) * 10)))  # Inverse similarity\n",
    "\n",
    "    return creativity_score, f\"Novelty score: {1 - max_similarity:.2f} (Lower similarity = more creative)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON dataset\n",
    "with open(\"human_bot_conversation.json\", \"r\") as file:\n",
    "    conversations = json.load(file)\n",
    "\n",
    "results = []\n",
    "past_responses = []  # Stores previous bot responses for creativity evaluation\n",
    "\n",
    "for idx, conversation_data in enumerate(conversations, 1):\n",
    "    conversation_text = conversation_data.get(\"conversation\", \"\")\n",
    "    turns = conversation_text.split(\"\\n\\n\")  # Split turns by double newlines\n",
    "\n",
    "    # Extract all user prompts and bot responses\n",
    "    user_prompts = [\n",
    "        t.replace(\"HUMAN:\", \"\").replace(\"[HUMAN]:\", \"\").strip() \n",
    "        for t in turns if \"HUMAN:\" in t or \"[HUMAN]:\" in t\n",
    "    ]\n",
    "    bot_responses = [\n",
    "        t.replace(\"BOT:\", \"\").replace(\"[BOT]:\", \"\").strip() \n",
    "        for t in turns if \"BOT:\" in t or \"[BOT]:\" in t\n",
    "    ]\n",
    "\n",
    "    # Extract images from JSON (if present)\n",
    "    image_tag_mapping = conversation_data.get(\"image_tag_mapping\", {})\n",
    "    encoded_images = conversation_data.get(\"images\", [])\n",
    "\n",
    "    # Apply evaluations\n",
    "    relevance_score, relevance_explanation = evaluate_relevance(user_prompts, bot_responses)\n",
    "    coherence_score, coherence_explanation = evaluate_coherence(conversation_text)\n",
    "    factual_accuracy_score, factual_accuracy_explanation = evaluate_factual_accuracy(bot_responses)\n",
    "    bias_toxicity_score, bias_toxicity_explanation = evaluate_bias_toxicity(bot_responses)\n",
    "    fluency_score, fluency_explanation = evaluate_fluency(bot_responses)\n",
    "    image_alignment_score, image_alignment_explanation = evaluate_image_alignment(bot_responses, encoded_images, image_tag_mapping)\n",
    "    creativity_score, creativity_explanation = evaluate_creativity(bot_responses, past_responses)\n",
    "\n",
    "    # Store past responses for creativity comparison\n",
    "    past_responses.extend(bot_responses)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"conversation_id\": idx,\n",
    "        \"evaluation_scores\": {\n",
    "            \"Relevance\": {\"score\": relevance_score, \"explanation\": relevance_explanation},\n",
    "            \"Coherence\": {\"score\": coherence_score, \"explanation\": coherence_explanation},\n",
    "            \"Factual Accuracy\": {\"score\": factual_accuracy_score, \"explanation\": factual_accuracy_explanation},\n",
    "            \"Bias & Toxicity\": {\"score\": bias_toxicity_score, \"explanation\": bias_toxicity_explanation},\n",
    "            \"Fluency\": {\"score\": fluency_score, \"explanation\": fluency_explanation},\n",
    "            \"Image Alignment\": {\"score\": image_alignment_score, \"explanation\": image_alignment_explanation},\n",
    "            \"Creativity\": {\"score\": creativity_score, \"explanation\": creativity_explanation}\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save JSON results\n",
    "with open(\"Final_Conversation_Evaluation.json\", \"w\") as output_file:\n",
    "    json.dump(results, output_file, indent=4)\n",
    "\n",
    "print(\"Evaluation completed. Results saved to 'Final_Conversation_Evaluation.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
